{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get Text data of wikipedia\n",
    "\n",
    "When you start to get in Deep Learning or Machine Learning, you must have data to train and test with your machin learning algorithm. \n",
    "\n",
    "Basically, MNIST data or CIFAR-10 and so forth are given with Deap Learning and Machine learning framework like [Tensorflow](https://www.tensorflow.org/).\n",
    "\n",
    "Even in tensorflow You easily get data of MNIST by calling some function like this. \n",
    "\n",
    "```python\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "```\n",
    "\n",
    "If you know how to get MNIST data with Tensorflow, refer to [MNIST of Tensorflow](https://www.tensorflow.org/get_started/mnist/beginners#the_mnist_data) or My blog(hyunyoung2.github.io)'s [Howt get and use MNIST in Tensorflow](https://hyunyoung2.github.io/2018/01/18/How_To_Get_And_Use_MNIST_In_Tensorflow/)\n",
    "\n",
    "BUT, If you want to ust text data, Normally You have to do crawling or get from someone who has the data you want. \n",
    "\n",
    "In the case of crawling, you also have to parsing the data which you crawled, That is so annoying. \n",
    "\n",
    "So I will introduce more easy way to get text data with wikipedia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to Download and Parse wikipedia. \n",
    "\n",
    "\n",
    "Wikipedia provide text dump file per languages. Because that data is given as xml format.\n",
    "\n",
    "you have to parse the xml after download. \n",
    "\n",
    "If you want to download wikimedia file, vist [here, wikimedia](https://dumps.wikimedia.org).\n",
    "\n",
    "In my case, I will deal with Korean language. So If you want to download Korean language file, vist [kowiki](https://dumps.wikimedia.org/kowiki/)\n",
    "\n",
    "![](https://raw.githubusercontent.com/hyunyoung2/hyunyoung2_Machine_Learning/master/Tutorial/Tensorflow/02.DataSetting/images/TEXT/wikimedia_kowiki.png)\n",
    "\n",
    "After getting in the URL, click date you want to download. \n",
    "\n",
    "wikimedia provide a variety of dump files as xml. Basically, I think it is enough to use pages-article.xml.bz2 dump file among the dump files for you.\n",
    "\n",
    "![](https://raw.githubusercontent.com/hyunyoung2/hyunyoung2_Machine_Learning/master/Tutorial/Tensorflow/02.DataSetting/images/TEXT/wikimedia_20180101.png)\n",
    "\n",
    "When I made word2vec. I used it from https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2  \n",
    "\n",
    "And If you want to download another language, Just change **ko** to thing you want in kowiki. i.e ko indicates korean. as en short for english. likewise utilize some word which represents another country language.\n",
    "\n",
    "When you finish downloading dump file of wikimedia. you have to parse it. But don't worry about it. \n",
    "\n",
    "Already someone made it to parse wikimedia file as opensource. I recommend you a opensource, [Wikiextractor's github](https://github.com/attardi/wikiextractor).\n",
    "\n",
    "The way to use the opensource is \"git clone https://github.com/attardi/wikiextractor.git\"\n",
    "\n",
    "\n",
    "# [Wikiextractor](https://github.com/attardi/wikiextractor)\n",
    "\n",
    "The following is the result of running wikiextractor.py with wikimedia dump file.\n",
    "\n",
    "The resulting file is json or plain text formatted in a number of files of simmilar size in a given directory.\n",
    "\n",
    "\n",
    "##### python3 Wikiextractor.py --json \n",
    "\n",
    "\n",
    "If you invoke wikiextractor.py with --json flag. the resulting format is  json object one per line with the following structure.\n",
    "\n",
    "```\n",
    "    {\"id\": \"\", \"revid\": \"\", \"url\":\"\", \"title\": \"\", \"text\": \"...\"}\n",
    "```\n",
    "\n",
    "The following is a portion of wiki_00 under AA directory\n",
    "\n",
    "![](https://raw.githubusercontent.com/hyunyoung2/hyunyoung2_Machine_Learning/master/Tutorial/Tensorflow/02.DataSetting/images/TEXT/wikiextractor_json.png)\n",
    "\n",
    "\n",
    "##### python3 Wikiextractor.py \n",
    "\n",
    "If you invoke Wikiextractor with no flag, Each file will contain several documents in the format:\n",
    "\n",
    "```\n",
    "<doc id=\"\" revid=\"\" url=\"\" title=\"\">\n",
    "        content(text\n",
    "        </doc>\n",
    "```\n",
    "\n",
    "The following is a portion of wiki_00 under AA directory\n",
    "\n",
    "![](https://github.com/hyunyoung2/hyunyoung2_Machine_Learning/blob/master/Tutorial/Tensorflow/02.DataSetting/images/TEXT/wikiextractor_basic.png)\n",
    "\n",
    "The following is the case where you invoke Wikiextractor.py with -l(link), --lists, or -s(section) flag\n",
    "\n",
    "##### python3 Wikiextractor.py  -l\n",
    "\n",
    "-l means link, so after parsing, there is link such as a tag of html\n",
    "\n",
    "The following is a portion of wiki_00 under AA directory\n",
    "\n",
    "![](https://raw.githubusercontent.com/hyunyoung2/hyunyoung2_Machine_Learning/master/Tutorial/Tensorflow/02.DataSetting/images/TEXT/wikiextractor_link.png)\n",
    "\n",
    "##### python3 Wikiextractor.py  --lists\n",
    "\n",
    "--lists literally means list, so after parsing, there is list of contents of wikimedia.\n",
    "\n",
    "The following is a portion of wiki_00 under AA directory\n",
    "\n",
    "![](https://raw.githubusercontent.com/hyunyoung2/hyunyoung2_Machine_Learning/master/Tutorial/Tensorflow/02.DataSetting/images/TEXT/wikiextractor_lists.png)\n",
    "\n",
    "##### python3 Wikiextractor.py  -s \n",
    "\n",
    "-s means section, i.e. subtitle remaint as parsed. \n",
    "\n",
    "The following is a portion of wiki_00 under AA directory\n",
    "\n",
    "![](https://raw.githubusercontent.com/hyunyoung2/hyunyoung2_Machine_Learning/master/Tutorial/Tensorflow/02.DataSetting/images/TEXT/wikiExtractor_section.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "\n",
    " - [Wikiextractor's github](https://github.com/attardi/wikiextractor)\n",
    " - [doc2vec-api](https://github.com/roboreport/doc2vec-api/)\n",
    " - [someone's blog](http://kugancity.tistory.com/entry/%ED%95%9C%EA%B5%AD%EC%96%B4-%EC%9C%84%ED%82%A4%ED%94%BC%EB%94%94%EC%95%84-%EB%8D%A4%ED%94%84-%EB%8B%A4%EC%9A%B4%EB%A1%9C%EB%93%9C-%EB%B0%9B%EA%B8%B0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
