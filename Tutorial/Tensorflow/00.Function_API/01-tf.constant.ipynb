{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [tf.constant](https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/constant)\n",
    "\n",
    "This function is for constant tensor, If you want to make constant tensor which is not updated by updating algorithm like backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Version checking ===\n",
      "The version of sys: \n",
      "3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609]\n",
      "Tensorflow version: 1.8.0\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Example code for tf.constant(https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/constant)\n",
    "\n",
    "tf.constant(\n",
    "    value,\n",
    "    dtype=None,\n",
    "    shape=None,\n",
    "    name='Const',\n",
    "    verify_shape=False\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"=== Version checking ===\")\n",
    "print(\"The version of sys: \\n{}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "print(\"========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Tensor Shape ======\n",
      "tensor1: Tensor(\"Const:0\", shape=(7,), dtype=int32)\n",
      "tensor2: Tensor(\"Const_1:0\", shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Constant 1-D Tensor populated with value list.\n",
    "tensor1 = tf.constant([1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "# Constant 2-D tensor populated with scalar value -1.\n",
    "tensor2 = tf.constant(-1.0, shape=[2, 3]) \n",
    "\n",
    "print(\"===== Tensor Shape ======\")\n",
    "print(\"tensor1: {}\".format(tensor1))\n",
    "print(\"tensor2: {}\".format(tensor2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor1: \n",
      "[1 2 3 4 5 6 7], The shape: (7,)\n",
      "tensor2: \n",
      "[[-1. -1. -1.]\n",
      " [-1. -1. -1.]], The shape: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(\"tensor1: \\n{}, The shape: {}\".format(sess.run(tensor1), tensor1.shape))\n",
    "    print(\"tensor2: \\n{}, The shape: {}\".format(sess.run(tensor2), tensor2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it out whether the constant is updated by backpropagation.\n",
    "\n",
    "But you would get error message like \n",
    "\n",
    "> ValueError: No variables to optimize.\n",
    "\n",
    "That is because on the your defuat graph, you doesn't have variable trainable. \n",
    "\n",
    "Let's check below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables to optimize.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d4a7db2e23d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mglobal_step_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Global_setp1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0minit_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0mprocessors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to optimize.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0mvar_refs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     grads = gradients.gradients(\n",
      "\u001b[0;31mValueError\u001b[0m: No variables to optimize."
     ]
    }
   ],
   "source": [
    "x = [[1., 2., 3.],\n",
    "     [4., 5., 6.]]\n",
    "w = [[1., 2.],\n",
    "     [3., 4.],\n",
    "     [5., 6.]]\n",
    "b = [1., 1.]\n",
    "\n",
    "\n",
    "label = [[1., 0.], [0., 1.]]\n",
    "\n",
    "with tf.name_scope(\"Constant_variables\") as scope:\n",
    "    x_var = tf.constant(x, dtype=tf.float32)\n",
    "    # if you change the variable of weight and bias, \n",
    "    # Then you can run this code \n",
    "    weight = tf.constant(w, dtype=tf.float32)\n",
    "    bias = tf.constant(b, dtype=tf.float32)\n",
    "    ground_truths = tf.constant(label, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"Name_scope\") as scope:\n",
    "    output = tf.add(tf.matmul(x, weight), bias)\n",
    "    \n",
    "with tf.name_scope(\"Loss\") as scope:\n",
    "    sub_for_loss = tf.subtract(ground_truths, output)\n",
    "    losses = tf.reduce_mean(tf.square(sub_for_loss))\n",
    "    tf.summary.scalar(\"Loss\", losses)\n",
    "    \n",
    "with tf.name_scope(\"Training\") as scope:\n",
    "    global_step_var = tf.Variable(0, name=\"Global_setp1\", trainable=False)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(losses, global_step=global_step_var)\n",
    "    \n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "merged_op = tf.summary.merge_all()\n",
    "\n",
    "print(\"===== Tensor Shape ======\")\n",
    "print(\"x_var: {}\".format(x_var))\n",
    "print(\"weight: {}\".format(weight))\n",
    "print(\"bias: {}\".format(bias))\n",
    "print(\"ground_truths: {}\".format(ground_truths))\n",
    "print(\"output: {}\".format(output))\n",
    "print(\"losses: {}\".format(losses))\n",
    "print(\"global_step_var: {}\".format(global_step_var))\n",
    "print(\"optimizer: {}\".format(optimizer))\n",
    "print(\"train_op: {}\".format(train_op))\n",
    "print(\"init_op: {}\".format(init_op))\n",
    "print(\"merged_op: {}\".format(merged_op))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check with **tf.get_collections** function in detail about what kind of variables is on your graph. \n",
    "\n",
    "the code above only has variable named \"Training/Global_step1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== variable type =====\n",
      "\n",
      "tf.GraphKeys.GLOBAL_VARIABLES: [<tf.Variable 'Training/Global_setp1:0' shape=() dtype=int32_ref>]\n",
      "\n",
      "tf.GraphKeys.TRAINABLE_VARIABLES: []\n",
      "\n",
      "tf.GraphKeys.LOCAL_VARIABLES: []\n",
      "\n",
      "===== all variables =====\n",
      "Training/Global_setp1:0\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#    Graph's Checking variable    #\n",
    "###################################\n",
    "trainable_variable1 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "trainable_variable2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "trainable_variable3 = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES)\n",
    "print(\"===== variable type =====\")\n",
    "print(\"\\ntf.GraphKeys.GLOBAL_VARIABLES: {}\".format(trainable_variable1))\n",
    "print(\"\\ntf.GraphKeys.TRAINABLE_VARIABLES: {}\".format(trainable_variable2))\n",
    "print(\"\\ntf.GraphKeys.LOCAL_VARIABLES: {}\".format(trainable_variable3))\n",
    "print(\"\\n===== all variables =====\")\n",
    "for v in tf.global_variables():\n",
    "    print(v.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, Let's change your graph to be trainable. \n",
    "\n",
    "First of all, change the weight and bias with tf.Variable like this :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Tensor Shape ======\n",
      "x_var: Tensor(\"Constant_variables/Const:0\", shape=(2, 3), dtype=float32)\n",
      "weight: <tf.Variable 'Constant_variables/Variable:0' shape=(3, 2) dtype=float32_ref>\n",
      "bias: <tf.Variable 'Constant_variables/Variable_1:0' shape=(2,) dtype=float32_ref>\n",
      "ground_truths: Tensor(\"Constant_variables/Const_1:0\", shape=(2, 2), dtype=float32)\n",
      "output: Tensor(\"Name_scope/Add:0\", shape=(2, 2), dtype=float32)\n",
      "losses: Tensor(\"Loss/Mean:0\", shape=(), dtype=float32)\n",
      "global_step_var: <tf.Variable 'Training/Global_setp:0' shape=() dtype=int32_ref>\n",
      "optimizer: <tensorflow.python.training.gradient_descent.GradientDescentOptimizer object at 0x7f7357ff2a90>\n",
      "train_op: name: \"Training/GradientDescent\"\n",
      "op: \"AssignAdd\"\n",
      "input: \"Training/Global_setp\"\n",
      "input: \"Training/GradientDescent/value\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_INT32\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_class\"\n",
      "  value {\n",
      "    list {\n",
      "      s: \"loc:@Training/Global_setp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"use_locking\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "\n",
      "init_op: name: \"init\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Constant_variables/Variable/Assign\"\n",
      "input: \"^Constant_variables/Variable_1/Assign\"\n",
      "input: \"^Training/Global_setp/Assign\"\n",
      "\n",
      "merged_op: Tensor(\"Merge/MergeSummary:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "x = [[1., 2., 3.],\n",
    "     [4., 5., 6.]]\n",
    "w = [[1., 2.],\n",
    "     [3., 4.],\n",
    "     [5., 6.]]\n",
    "b = [1., 1.]\n",
    "\n",
    "\n",
    "label = [[1., 0.], [0., 1.]]\n",
    "\n",
    "with tf.name_scope(\"Constant_variables\") as scope:\n",
    "    x_var = tf.constant(x, dtype=tf.float32)\n",
    "    # if you change the variable of weight and bias, \n",
    "    # Then you can run this code \n",
    "    weight = tf.Variable(w, dtype=tf.float32)\n",
    "    bias = tf.Variable(b, dtype=tf.float32)\n",
    "    ground_truths = tf.constant(label, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"Name_scope\") as scope:\n",
    "    output = tf.add(tf.matmul(x, weight), bias)\n",
    "    \n",
    "with tf.name_scope(\"Loss\") as scope:\n",
    "    sub_for_loss = tf.subtract(ground_truths, output)\n",
    "    losses = tf.reduce_mean(tf.square(sub_for_loss))\n",
    "    tf.summary.scalar(\"Loss\", losses)\n",
    "    \n",
    "with tf.name_scope(\"Training\") as scope:\n",
    "    global_step_var = tf.Variable(0, name=\"Global_setp\", trainable=False)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    train_op = optimizer.minimize(losses, global_step=global_step_var)\n",
    "    \n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "merged_op = tf.summary.merge_all()\n",
    "\n",
    "print(\"===== Tensor Shape ======\")\n",
    "print(\"x_var: {}\".format(x_var))\n",
    "print(\"weight: {}\".format(weight))\n",
    "print(\"bias: {}\".format(bias))\n",
    "print(\"ground_truths: {}\".format(ground_truths))\n",
    "print(\"output: {}\".format(output))\n",
    "print(\"losses: {}\".format(losses))\n",
    "print(\"global_step_var: {}\".format(global_step_var))\n",
    "print(\"optimizer: {}\".format(optimizer))\n",
    "print(\"train_op: {}\".format(train_op))\n",
    "print(\"init_op: {}\".format(init_op))\n",
    "print(\"merged_op: {}\".format(merged_op))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with **tf.get_collection** function, you could see what kind of variables you have to update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== variable type =====\n",
      "\n",
      "tf.GraphKeys.GLOBAL_VARIABLES: [<tf.Variable 'Constant_variables/Variable:0' shape=(3, 2) dtype=float32_ref>, <tf.Variable 'Constant_variables/Variable_1:0' shape=(2,) dtype=float32_ref>, <tf.Variable 'Training/Global_setp:0' shape=() dtype=int32_ref>]\n",
      "\n",
      "tf.GraphKeys.TRAINABLE_VARIABLES: [<tf.Variable 'Constant_variables/Variable:0' shape=(3, 2) dtype=float32_ref>, <tf.Variable 'Constant_variables/Variable_1:0' shape=(2,) dtype=float32_ref>]\n",
      "\n",
      "tf.GraphKeys.LOCAL_VARIABLES: []\n",
      "\n",
      "===== all variables =====\n",
      "Constant_variables/Variable:0\n",
      "Constant_variables/Variable_1:0\n",
      "Training/Global_setp:0\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#    Graph's Checking variable    #\n",
    "###################################\n",
    "trainable_variable1 = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "trainable_variable2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "trainable_variable3 = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES)\n",
    "print(\"===== variable type =====\")\n",
    "print(\"\\ntf.GraphKeys.GLOBAL_VARIABLES: {}\".format(trainable_variable1))\n",
    "print(\"\\ntf.GraphKeys.TRAINABLE_VARIABLES: {}\".format(trainable_variable2))\n",
    "print(\"\\ntf.GraphKeys.LOCAL_VARIABLES: {}\".format(trainable_variable3))\n",
    "print(\"\\n===== all variables =====\")\n",
    "for v in tf.global_variables():\n",
    "    print(v.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run you graph, the varialbe which is be trainable on your default graph is traned after run train_op variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_var, constant: \n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]], The shape: (2, 3)\n",
      "weight: \n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]], The shape: (3, 2)\n",
      "bias: \n",
      "[1. 1.], The shape: (2,)\n",
      "ground_truths, constant: \n",
      "[[1. 0.]\n",
      " [0. 1.]], The shape: (2, 2)\n",
      "output: \n",
      "[[23. 29.]\n",
      " [50. 65.]], The shape: (2, 2)\n",
      "sub_for_loss: \n",
      "[[-22. -29.]\n",
      " [-50. -64.]], The shape: (2, 2)\n",
      "losses: \n",
      "1980.25, The shape: ()\n",
      "global_step_var: \n",
      "0, The shape: ()\n",
      "================== from now on, print one more time =========================\n",
      "x_var: \n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      ", weight: \n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n",
      ", bias: \n",
      "[1. 1.]\n",
      ", \n",
      "        ground_truths: \n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      ", output: \n",
      "[[23. 29.]\n",
      " [50. 65.]]\n",
      ", sub_for_loss: \n",
      "[[-22. -29.]\n",
      " [-50. -64.]]\n",
      ", \n",
      "        global_step_var: \n",
      "1980.25\n",
      "\n",
      "====================================================\n",
      "====================== after training ==============================\n",
      "x_var: \n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      ", weight: \n",
      "[[-10.1      -12.25    ]\n",
      " [-11.7      -14.900001]\n",
      " [-13.3      -17.550001]]\n",
      ", bias: \n",
      "[-2.6000001 -3.65     ]\n",
      ", \n",
      "        ground_truths: \n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      ", output: \n",
      "[[ -76.        -98.350006]\n",
      " [-181.30002  -232.45    ]]\n",
      ", sub_for_loss: \n",
      "[[ 77.        98.350006]\n",
      " [181.30002  233.45    ]]\n",
      ", \n",
      "        global_step_var: \n",
      "25742.580078125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    train_writer =  tf.summary.FileWriter(\"./01-tf.constant\", sess.graph)\n",
    "    for _ in range(1):\n",
    "        print(\"x_var, constant: \\n{}, The shape: {}\".format(sess.run(x_var), x_var.shape))\n",
    "        print(\"weight: \\n{}, The shape: {}\".format(sess.run(weight), weight.shape))\n",
    "        print(\"bias: \\n{}, The shape: {}\".format(sess.run(bias), bias.shape))\n",
    "        print(\"ground_truths, constant: \\n{}, The shape: {}\".format(sess.run(ground_truths), ground_truths.shape))\n",
    "        print(\"output: \\n{}, The shape: {}\".format(sess.run(output), output.shape))\n",
    "        print(\"sub_for_loss: \\n{}, The shape: {}\".format(sess.run(sub_for_loss), sub_for_loss.shape))\n",
    "        print(\"losses: \\n{}, The shape: {}\".format(sess.run(losses), losses.shape))\n",
    "        print(\"global_step_var: \\n{}, The shape: {}\".format(sess.run(global_step_var), global_step_var.shape))\n",
    "    \n",
    "        print(\"================== from now on, print one more time =========================\")\n",
    "        x_var_, weight_, bias_, ground_truths_, output_, sub_for_loss_, losses_, global_step_var_, _ = sess.run([x_var, weight,\n",
    "                                                                                                         bias, ground_truths,\n",
    "                                                                                                         output, sub_for_loss,\n",
    "                                                                                                         losses, global_step_var,\n",
    "                                                                                                         train_op])\n",
    "        print(\"\"\"x_var: \\n{}\\n, weight: \\n{}\\n, bias: \\n{}\\n, \n",
    "        ground_truths: \\n{}\\n, output: \\n{}\\n, sub_for_loss: \\n{}\\n, \n",
    "        global_step_var: \\n{}\\n\"\"\".format(x_var_, weight_, bias_, ground_truths_, output_, sub_for_loss_, losses_, global_step_var_))\n",
    "        print(\"====================================================\")\n",
    "        x_var_, weight_, bias_, ground_truths_, output_, sub_for_loss_, losses_, global_step_var_ = sess.run([x_var, weight,\n",
    "                                                                                                         bias, ground_truths,\n",
    "                                                                                                         output, sub_for_loss,\n",
    "                                                                                                         losses, global_step_var])\n",
    "        print(\"====================== after training ==============================\")\n",
    "        print(\"\"\"x_var: \\n{}\\n, weight: \\n{}\\n, bias: \\n{}\\n, \n",
    "        ground_truths: \\n{}\\n, output: \\n{}\\n, sub_for_loss: \\n{}\\n, \n",
    "        global_step_var: \\n{}\\n\"\"\".format(x_var_, weight_, bias_, ground_truths_, output_, sub_for_loss_, losses_, global_step_var_))\n",
    "    \n",
    "    \n",
    "    train_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "\n",
    " - [tf. constant](https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/constant) in Tensorflow apidoc from version r1.8 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
