{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is the word embedding?\n",
    "\n",
    "An Embedding is a mapping from discrete object into vectors of real numbers. It is important to use text on task of Natural lanugauges processing, in particular, Machine learning and Deep Learning. For an example, When you embed words into vector space, It looks as follows:\n",
    "\n",
    "```\n",
    "blue:  (0.01359, 0.00075997, 0.24608, ..., -0.2524, 1.0048, 0.06259)\n",
    "blues:  (0.01396, 0.11887, -0.48963, ..., 0.033483, -0.10007, 0.1158)\n",
    "orange:  (-0.24776, -0.12359, 0.20986, ..., 0.079717, 0.23865, -0.014213)\n",
    "oranges:  (-0.35609, 0.21854, 0.080944, ..., -0.35413, 0.38511, -0.070976)\n",
    "```\n",
    "\n",
    "In case of vetor of word embedding, Specifically, We don't know what each dimension in a vector means. However, We can realize some features from overall pattern of location and distance between vectors that Machine Learning take advantage of. \n",
    "\n",
    "Because words of text can be represented as natural vectors,the way of word embedding is standard and effective way to transform such discrete. depending on how dense it is, and how better it utilize on the task that need vector of words of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How to embed word in Tensorflow\n",
    "\n",
    "To create word embeddings in Tensorflow, we first split the text into words and then we assign integer number to every word in every vocabulary as follow\n",
    "\n",
    "```python\n",
    "text = \"I have a cat.\"\n",
    "# array has Index of each words.The Index is word_ids \n",
    "# Index:  0,  1  , 2,  3 , 4  --> word_ids\n",
    "# Array: [I, have, a, cat, .]\n",
    "array_text = [I, have, a, cat, .]\n",
    "```\n",
    "\n",
    "As you can see above, We need to map **word_ids** to **vectors** in Tensorflow. For this situation, use the following functions :\n",
    "\n",
    "```python\n",
    "word_embeddings = tf.get_variable(\"word_embeddings\", [vocabulary_size, embedding_size])\n",
    "embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, word_ids)\n",
    "```\n",
    "\n",
    "The variable of **word_embeddings** is a array that is comprised of **total vocabulary size** and the dimesional size of each words we called **embedding size**. For example. \n",
    "\n",
    " - a sentence(a document) : I have a cat. \n",
    " - The total size of vocabularies : 5(I, have, a, cat, .)\n",
    " - The size of embedding : whatever size you want, 300-dimensional for each words that normally used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In detail, Let's walk through the function of **tf.nn.embedding_lookup**.\n",
    "\n",
    "```python\n",
    "word_embeddings = tf.get_variable(\"word_embeddings\", [vocabulary_size, embedding_size])\n",
    "embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, word_ids)\n",
    "```\n",
    "\n",
    "This function, **tf.nn.embedding_lookup**, literally means lookup in embeddings array depending on **word_ids**.\n",
    "\n",
    "let's say, we have word_emedddings matrix like this:\n",
    "\n",
    "```python\n",
    "# word embedding function of tensorflow\n",
    "# word_embeddings is 2 by 2 matrix, each factor is randomized. \n",
    "# [[00, 01],\n",
    "#  [10, 11]]\n",
    "word_embeddings = tf.get_variable(\"word_embeddings_test\", [2, 2])\n",
    "# In here, 1 mean the first row of word_embedding matrix. \n",
    "embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, ids=1)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    embedding_result_test, embedded_word_result_test = sess.run([word_embeddings, embedded_word_ids])\n",
    "    print(\"=== the test result ===\\n\")\n",
    "    print(\"The result of word_embedding:\\n\",embedding_result_test, \"\\n\")\n",
    "    print(\"The result of embedded_word_ids:\\n\",embedded_word_result_test, \"\\n\")\n",
    "```\n",
    "\n",
    "If you execute the graph above, the result of **tf.nn.embedding_lookup(matrix, ids)** is rows equal to **ids**. Let's say ids set **1** as ids. the 1 index row of **word_embeddings** matrix  is returned. If you enter ids such as a list([0, 1]), the return value is a list including row value of **word_embeddings** matrix depending the factors of list. \n",
    "\n",
    "  - If ids is 1, the return value is word_embedding[1]\n",
    "  \n",
    "  - If ids is [0, 1], the return value is [word_embedding[0], word_embedding[1]]\n",
    "  \n",
    "Let's see the result of computation graph above:\n",
    "\n",
    "```shell\n",
    "=== the test result ===\n",
    "\n",
    "The result of word_embedding:\n",
    " [[-0.7246539   0.43106437]\n",
    " [ 0.52070427  1.06491148]] \n",
    "\n",
    "The result of embedded_word_ids:\n",
    " [ 0.52070427  1.06491148] \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use Tensorflow projector as debugging. \n",
    "\n",
    "When you are embedding text or image with Tensorflow, Tensorflow provide great tool to help you easily debug. It is calle Tensorboard.\n",
    "\n",
    "[Tensorboard](https://www.youtube.com/embed/eBbEDRsCmv4) is great tool. that draws your graph of computation and help you check some value of your model like FeedForward Neural Network. But, here you walk through how to project embedding matrix into 2-D ro 3-D.\n",
    "\n",
    "let's see an simple example code:\n",
    "\n",
    "First is no label \n",
    "\n",
    "```python \n",
    "# How to use tensorboard for WordEmbedding. \n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "LOG_DIR = os.path.join(os.getcwd(),\"log/\")\n",
    "\n",
    "print(\"the current working directory:\", LOG_DIR, \"\\n\")\n",
    "\n",
    "# word vector of embedding. \n",
    "embedding_temp = tf.Variable([[0.1,0.2],[0.5,0.5]], dtype=tf.float32, name=\"test_embedding-no-label\")\n",
    "\n",
    "# To initilize the variable. \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# To add ops to save and restore all the variable\n",
    "# When you want to make it\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    # print embedding_temp\n",
    "    embedding_temp_ = sess.run(embedding_temp)\n",
    "    print(embedding_temp_)\n",
    "    \n",
    "    save_path = saver.save(sess, os.path.join(LOG_DIR, \"test_embedding_model.ckpt\"), global_step=0)\n",
    "    print(\"\\nModel Saved in file: %s\" % save_path)\n",
    "```\n",
    "\n",
    "In here, you have to save checkpoint file to save all the variables on your model. if you completely save checkpoint file, move to directory you saved checkpoint file to. \n",
    "\n",
    "and then prompt in command line like this:\n",
    "\n",
    "> tensorboard --logdir=the-path-you-saved-checkpoint-file-to\n",
    "\n",
    "the result of the above code is: \n",
    "\n",
    "![](https://raw.githubusercontent.com/hyunyoung2/hyunyoung2_Machine_Learning/master/Tutorial/Tensorflow/03.Word2Vec/images/01.Word_Embedding/Word_Embedding_With_no_label_on_tensorboard.png)\n",
    "\n",
    "As you can see the code, there is no label for each words. so on Tensorboard Projector, each words are represented as index number.\n",
    "\n",
    "Let's say another one using with lable of embedding. In particular, here I tested it for NLP. \n",
    "\n",
    "```python\n",
    "# How to use tensorboard for WordEmbedding. \n",
    "# with metadata file \n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "LOG_DIR = os.path.join(os.getcwd(),\"log/\")\n",
    "\n",
    "print(\"the current working directory:\", LOG_DIR, \"\\n\")\n",
    "\n",
    "LABEL_FILE = os.path.join(os.getcwd(), \"log/lables.tsv\")\n",
    "\n",
    "print(\"the current LABEL_FILE:\", LABEL_FILE, \"\\n\")\n",
    "\n",
    "LABEL_NUM = 2\n",
    "LABEL = [\"NLP\", \"Deep Learning\"]\n",
    "\n",
    "# Write label file \n",
    "with open(LABEL_FILE,\"w\") as f:\n",
    "    for i in range(LABEL_NUM):\n",
    "        f.write(LABEL[i]+\"\\n\")\n",
    "    print(\"labels.tsv file Created!\\n\")\n",
    "\n",
    "# word vector variable to embedding. \n",
    "embedding_temp = tf.Variable([[0.1,0.2],[0.5,0.5]], dtype=tf.float32, name=\"test_embedding\")\n",
    "\n",
    "# To initilize the variable. \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# To add ops to save and restore all the variable\n",
    "# When you want to make it\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    # print embedding_temp\n",
    "    embedding_temp_ = sess.run(embedding_temp)\n",
    "    print(embedding_temp_)\n",
    "    \n",
    "    # Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    # You can add multiple embeddings. Here we add only one.\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding_temp.name\n",
    "    # Link this tensor to its metadata file (e.g. labels).\n",
    "    embedding_config.metadata_path = LABEL_FILE\n",
    "    \n",
    "    # Use the same LOG_DIR where you stored your checkpoint.\n",
    "    summary_writer = tf.summary.FileWriter(LOG_DIR)\n",
    "    \n",
    "    # The next line writes a projector_config.pbtxt in the LOG_DtenIR. TensorBoard will\n",
    "    # read this file during startup.\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(summary_writer, config)\n",
    "    \n",
    "    \n",
    "    save_path = saver.save(sess, os.path.join(LOG_DIR, \"test_embedding_model.ckpt\"), global_step=1)\n",
    "    print(\"\\nModel Saved in file: %s\" % save_path)\n",
    "```\n",
    "\n",
    "The different one from the former one is **tf.contrib.tensorboard.plugins.projector.ProjectorConfig()**.\n",
    "\n",
    "you have to set labal file(TSV file) up to Its return value(**config**) \n",
    "\n",
    "If you create label file, the format of the file isto write words line by line depending on word_embedding matrix like this:\n",
    "\n",
    "```\n",
    "NLP\\n\n",
    "Deep Lerning\\n\n",
    "```\n",
    "\n",
    "![](https://raw.githubusercontent.com/hyunyoung2/hyunyoung2_Machine_Learning/master/Tutorial/Tensorflow/03.Word2Vec/images/01.Word_Embedding/Word_Embedding_With_label_on_tensorboard.png)\n",
    "\n",
    "\n",
    "In order to use Tensorboard projector, First you need variable to represent embedding data like **embedding_temp** on the above codes. And then just save checkpoint file to save all the variable of your model. \n",
    "\n",
    "It  is all what you have to do for projector of embeddin onto Tensorboard. \n",
    "\n",
    "If you save checkpoint file, run the following:\n",
    "\n",
    "> tensorboard --logdir=the-path-you-saved-checkpoint-file-to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Reference \n",
    "\n",
    "   - [Tensorflow's Embeddings](https://www.tensorflow.org/programmers_guide/embedding)   \n",
    "\n",
    "   - [Tensorflow's how to use Tensorboard for Embedding r1.0](https://www.tensorflow.org/versions/r1.0/get_started/embedding_viz)\n",
    "      \n",
    "   - [Tensorflow's API of tf.nn.embedding_lookup](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup)  \n",
    "\n",
    "   - [Tensorflow's API of tf.gather](https://www.tensorflow.org/api_docs/python/tf/gather)  \n",
    "\n",
    "   - [Tensorflow's tensor](https://www.tensorflow.org/programmers_guide/tensors)  \n",
    "\n",
    "   - [Tensorflow's variable](https://www.tensorflow.org/programmers_guide/variables)  \n",
    "\n",
    "   - [an example code of Tensorboard](https://github.com/dandelionmane/tf-dev-summit-tensorboard-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version of python: 3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609] \n",
      "\n",
      "The version of tenserflow: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "# -- The version of python is 3.5.2---\n",
    "print(\"The version of python:\", sys.version, \"\\n\")\n",
    "# -- The version of tensorflow is 1.4 --\n",
    "print(\"The version of tenserflow:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2word: ['I', 'like', 'deep', 'flying', 'NLP', 'learning', 'enjoy'] \n",
      "\n",
      "word2id: {0: 'I', 1: 'like', 2: 'deep', 3: 'flying', 4: 'NLP', 5: 'learning', 6: 'enjoy'} \n",
      "\n",
      "count of word:\n",
      " [('I', 3), ('like', 2), ('deep', 1), ('flying', 1), ('NLP', 1), ('learning', 1), ('enjoy', 1)] \n",
      "\n",
      "the size of vocabularay: 7 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see how to use word embedding function in tensorflow\n",
    "import collections # for Counter class\n",
    "\n",
    "sample_string = [\"I like deep learning\", \n",
    "                \"I like NLP\",\n",
    "                \"I enjoy flying\"]\n",
    "\n",
    "vocabulary_size = 0 \n",
    "embbeding_size = 2\n",
    "# To build up dataset without duplication of words like this :\n",
    "# word2id, id2word, word_count, total vocab \n",
    "def build_up_dataset(sentences):\n",
    "    # merger a variety of strings into one string, delimiter=\" \"(white space) \n",
    "    words = \" \".join(sample_string).split()\n",
    "    # without duplication of words, each words couting. \n",
    "    counter = collections.Counter(words).most_common()\n",
    "    temp_list_id2word = [items[0] for items in counter] # idx-> word : search word from index\n",
    "    # search idx from word\n",
    "    dict_temp_word2id = {key:val for key, val in enumerate(temp_list_id2word)}\n",
    "    # total of vocabulary\n",
    "    voca_size = len(temp_list_id2word)\n",
    "    return temp_list_id2word, dict_temp_word2id, counter, voca_size\n",
    "    \n",
    "list_id2word, dict_word2id, count, vocabulary_size = build_up_dataset(sample_string)\n",
    "\n",
    "print(\"id2word:\", list_id2word, \"\\n\")\n",
    "print(\"word2id:\", dict_word2id, \"\\n\")\n",
    "print(\"count of word:\\n\", count, \"\\n\")\n",
    "print(\"the size of vocabularay:\", vocabulary_size, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== the test result ===\n",
      "\n",
      "The result of word_ids:\n",
      " [[[0 1]\n",
      "  [2 2]]] \n",
      "\n",
      "The result of word_embedding:\n",
      " [[-0.10185999 -0.59124708]\n",
      " [ 0.30318129 -0.42134538]\n",
      " [-0.68222558  0.21481109]\n",
      " [-0.14251018 -0.52396905]\n",
      " [-0.64809692 -0.79607165]\n",
      " [-0.36452004 -0.47127265]\n",
      " [ 0.04024273 -0.38521987]] \n",
      "\n",
      "The result of embedded_word_ids:\n",
      " [[[[-0.10185999 -0.59124708]\n",
      "   [ 0.30318129 -0.42134538]]\n",
      "\n",
      "  [[-0.68222558  0.21481109]\n",
      "   [-0.68222558  0.21481109]]]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# word embedding function of tensorflow\n",
    "word_ids= tf.placeholder(tf.int32, shape=[1,2,2])\n",
    "word_embeddings = tf.get_variable(\"word_embeddings\", [vocabulary_size, embbeding_size])\n",
    "embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, word_ids)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    word_ids, embedding_result, embedded_word_result = sess.run([word_ids, word_embeddings, embedded_word_ids], feed_dict={word_ids:[[[0,1],[2,2]]]})\n",
    "    print(\"=== the test result ===\\n\")\n",
    "    print(\"The result of word_ids:\\n\",word_ids, \"\\n\")\n",
    "    print(\"The result of word_embedding:\\n\",embedding_result, \"\\n\")\n",
    "    print(\"The result of embedded_word_ids:\\n\",embedded_word_result, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== the test result ===\n",
      "\n",
      "The result of word_embedding:\n",
      " [[ 0.0130285  -1.20233023]\n",
      " [-0.60363257  0.98856938]] \n",
      "\n",
      "The result of embedded_word_ids:\n",
      " [-0.60363257  0.98856938] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# word embedding function of tensorflow\n",
    "word_embeddings = tf.get_variable(\"word_embeddings_test\", [2, 2])\n",
    "embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, 1)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    embedding_result_test, embedded_word_result_test = sess.run([word_embeddings, embedded_word_ids])\n",
    "    print(\"=== the test result ===\\n\")\n",
    "    print(\"The result of word_embedding:\\n\",embedding_result_test, \"\\n\")\n",
    "    print(\"The result of embedded_word_ids:\\n\",embedded_word_result_test, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current working directory: /home/hyunyoung2/My_lab/hyunyoung2_Machine_Learning/Tutorial/Tensorflow/03.Word2Vec/log/ \n",
      "\n",
      "[[ 0.1  0.2]\n",
      " [ 0.5  0.5]]\n",
      "\n",
      "Model Saved in file: /home/hyunyoung2/My_lab/hyunyoung2_Machine_Learning/Tutorial/Tensorflow/03.Word2Vec/log/test_embedding_model.ckpt-0\n"
     ]
    }
   ],
   "source": [
    "# How to use tensorboard for WordEmbedding. \n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "LOG_DIR = os.path.join(os.getcwd(),\"log/\")\n",
    "\n",
    "print(\"the current working directory:\", LOG_DIR, \"\\n\")\n",
    "\n",
    "# word vector of embedding. \n",
    "embedding_temp = tf.Variable([[0.1,0.2],[0.5,0.5]], dtype=tf.float32, name=\"test_embedding-no-label\")\n",
    "\n",
    "# To initilize the variable. \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# To add ops to save and restore all the variable\n",
    "# When you want to make it\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    # print embedding_temp\n",
    "    embedding_temp_ = sess.run(embedding_temp)\n",
    "    print(embedding_temp_)\n",
    "    \n",
    "    save_path = saver.save(sess, os.path.join(LOG_DIR, \"test_embedding_model.ckpt\"), global_step=0)\n",
    "    print(\"\\nModel Saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current working directory: /home/hyunyoung2/My_lab/hyunyoung2_Machine_Learning/Tutorial/Tensorflow/03.Word2Vec/log/ \n",
      "\n",
      "the current LABEL_FILE: /home/hyunyoung2/My_lab/hyunyoung2_Machine_Learning/Tutorial/Tensorflow/03.Word2Vec/log/lables.tsv \n",
      "\n",
      "labels.tsv file Created!\n",
      "\n",
      "[[ 0.1  0.2]\n",
      " [ 0.5  0.5]]\n",
      "\n",
      "Model Saved in file: /home/hyunyoung2/My_lab/hyunyoung2_Machine_Learning/Tutorial/Tensorflow/03.Word2Vec/log/test_embedding_model.ckpt-1\n"
     ]
    }
   ],
   "source": [
    "# How to use tensorboard for WordEmbedding. \n",
    "# with metadata file \n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "LOG_DIR = os.path.join(os.getcwd(),\"log/\")\n",
    "\n",
    "print(\"the current working directory:\", LOG_DIR, \"\\n\")\n",
    "\n",
    "LABEL_FILE = os.path.join(os.getcwd(), \"log/lables.tsv\")\n",
    "\n",
    "print(\"the current LABEL_FILE:\", LABEL_FILE, \"\\n\")\n",
    "\n",
    "LABEL_NUM = 2\n",
    "LABEL = [\"NLP\", \"Deep Learning\"]\n",
    "\n",
    "# Write label file \n",
    "with open(LABEL_FILE,\"w\") as f:\n",
    "    for i in range(LABEL_NUM):\n",
    "        f.write(LABEL[i]+\"\\n\")\n",
    "    print(\"labels.tsv file Created!\\n\")\n",
    "\n",
    "# word vector of embedding. \n",
    "embedding_temp = tf.Variable([[0.1,0.2],[0.5,0.5]], dtype=tf.float32, name=\"test_embedding\")\n",
    "\n",
    "# To initilize the variable. \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# To add ops to save and restore all the variable\n",
    "# When you want to make it\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    # print embedding_temp\n",
    "    embedding_temp_ = sess.run(embedding_temp)\n",
    "    print(embedding_temp_)\n",
    "    \n",
    "    # Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    # You can add multiple embeddings. Here we add only one.\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding_temp.name\n",
    "    # Link this tensor to its metadata file (e.g. labels).\n",
    "    embedding_config.metadata_path = LABEL_FILE\n",
    "    \n",
    "    # Use the same LOG_DIR where you stored your checkpoint.\n",
    "    summary_writer = tf.summary.FileWriter(LOG_DIR)\n",
    "    \n",
    "    # The next line writes a projector_config.pbtxt in the LOG_DtenIR. TensorBoard will\n",
    "    # read this file during startup.\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(summary_writer, config)\n",
    "    \n",
    "    \n",
    "    save_path = saver.save(sess, os.path.join(LOG_DIR, \"test_embedding_model.ckpt\"), global_step=1)\n",
    "    print(\"\\nModel Saved in file: %s\" % save_path)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
